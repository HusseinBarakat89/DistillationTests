# DistillationTests

we have reconstructed experiments to focus on the general distillation stage of the two-stage learning framework with a reduced dataset and used the magnitude of the losses as a proxy measure of the effectiveness of the methods.

## 1. Dataset
In the general distillation, we set the maximum sequence length to 128 and used a subsample of English Wikipedia as the text corpus. The dataset was tokenized twice using BERT tokenizer and TinyBERT tokenizer while the identicality of the two tokenized sequences was asserted.
## 2. Settings
The conducted experiments aimed to compare the quality of the distillation process between two different configurations; the first one is the distillation setting presented by Hinton et al. [7] based on logit loss difference, which will be referenced in the rest of the report as logit loss distillation; and the second setting is by calculating four different losses through the different hidden state representations. The four losses in the second setting are Embedding loss, Hidden loss, Attention Loss, and Logit loss. This setting will be mentioned in the rest of the report in the name of multi-Loss distillation mechanism. Figure 2 depicts the different losses in the inference phase.

The general architecture of the two distillation settings included passing the same data through TinyBERT as a student model and BERT as a teacher model and extract the representation of the same data between the two encoders and calculating the loss between these representations. The two distillation settings were instantiated from the same initial parameters for the student model. Forward hooks were used on both models to read the hidden representations in the forward pass. Due to the difference in the dimension of the latent space between BERT (768) and TinyBERT (312), a set of linear layers (with no activation functions) were used to scale up the hidden representations of TinyBERT to match the size of BERT’s hidden representations.

The difference between the two distillation techniques was quantified using the loss of the logit layer on both training dataset and test dataset as it will be depicted and discussed in the next section. In addition, the average gradient through the different layers was evaluated through training phase to evaluate the quality of learning process. As gradient decays in the earlier layers with the increase of the model’s depth, comparing the gradient average between the two distillation schemes with the same initialized model can indicate the propagation of information across the different layers.
